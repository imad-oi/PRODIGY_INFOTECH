{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "\n",
    "This project endeavors to create a sophisticated deep learning model with the capability to recognize food items accurately from images and estimate their calorie content. With the widespread use of smartphones and growing concern for health and nutrition, such a model could empower users to track their dietary intake more effectively, make informed food choices, and work towards their health and fitness objectives. Through the application of advanced machine learning techniques, including convolutional neural networks (CNNs) for image recognition and regression algorithms for calorie estimation, the model aims to offer users a convenient and accessible solution for managing their nutrition effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data \n",
    "#==============================================\n",
    "import tensorflow_datasets as tfds\n",
    "#Tensorflow\n",
    "#==============================================\n",
    "import tensorflow as tf \n",
    "\n",
    "#Keras\n",
    "#==============================================\n",
    "from keras.applications.resnet import preprocess_input\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D,Dense,MaxPooling2D,Conv2D,Flatten\n",
    "#Data Preprocessing \n",
    "#==============================================\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Warnings\n",
    "#==============================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_dataset, train_info = tfds.load('food101', split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data \n",
    "validation_dataset, validation_info = tfds.load('food101', split='validation', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize The Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = preprocess_input(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed_train = train_dataset.map(lambda x: preprocess_image(x['image'], x['label']))\n",
    "dataset_preprocessed_test=validation_dataset.map(lambda x:preprocess_image(x['image'],x['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75750\n",
      "25250\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_preprocessed_train))\n",
    "print(len(dataset_preprocessed_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of The Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single preprocessed image: (224, 224, 3)\n",
      "Shape of a single preprocessed label: ()\n"
     ]
    }
   ],
   "source": [
    "for image,label in dataset_preprocessed_train.take(1):\n",
    "    image_shape=image.shape\n",
    "    label_shspe=label.shape\n",
    "    \n",
    "print(\"Shape of a single preprocessed image:\", image_shape)   \n",
    "print(\"Shape of a single preprocessed label:\", label_shspe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(101, activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # You can adjust the batch size as needed\n",
    "batched_dataset_train = dataset_preprocessed_train.batch(batch_size)\n",
    "batched_dataset_test = dataset_preprocessed_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 111, 111, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 26, 26, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 12, 12, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               9437696   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 101)               51813     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9730341 (37.12 MB)\n",
      "Trainable params: 9730341 (37.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/2368 [==============================] - 1723s 727ms/step - loss: 4.7395 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 2/10\n",
      "2368/2368 [==============================] - 1362s 575ms/step - loss: 4.6402 - accuracy: 0.0084 - val_loss: 4.6159 - val_accuracy: 0.0099\n",
      "Epoch 3/10\n",
      "2368/2368 [==============================] - 1358s 573ms/step - loss: 4.6261 - accuracy: 0.0086 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 4/10\n",
      "2368/2368 [==============================] - 1355s 572ms/step - loss: 4.6159 - accuracy: 0.0087 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 5/10\n",
      "2368/2368 [==============================] - 1358s 574ms/step - loss: 4.6159 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 6/10\n",
      "2368/2368 [==============================] - 1372s 579ms/step - loss: 4.6159 - accuracy: 0.0084 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 7/10\n",
      "2368/2368 [==============================] - 1365s 576ms/step - loss: 4.6159 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 8/10\n",
      "2368/2368 [==============================] - 1376s 581ms/step - loss: 4.6159 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 9/10\n",
      "2368/2368 [==============================] - 1376s 581ms/step - loss: 4.6159 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n",
      "Epoch 10/10\n",
      "2368/2368 [==============================] - 1370s 578ms/step - loss: 4.6159 - accuracy: 0.0085 - val_loss: 4.6152 - val_accuracy: 0.0099\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(batched_dataset_train, epochs=10, validation_data=batched_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calories For Each food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "calorie_lookup = {\n",
    "    'apple_pie': 300,\n",
    "    'pizza': 800,\n",
    "    'spaghetti_bolognese': 450,\n",
    "    'hamburger': 600,\n",
    "    'fried_rice': 350,\n",
    "    'hot_dog': 300,\n",
    "    'sushi': 400,\n",
    "    'chocolate_cake': 500,\n",
    "    'steak': 700,\n",
    "    'grilled_salmon': 350,\n",
    "    'caesar_salad': 200,\n",
    "    'french_fries': 400,\n",
    "    'omelette': 350,\n",
    "    'pancakes': 350,\n",
    "    'cheeseburger': 750,\n",
    "    'donuts': 250,\n",
    "    'lobster_bisque': 450,\n",
    "    'club_sandwich': 600,\n",
    "    'macaroni_and_cheese': 400,\n",
    "    'pad_thai': 500,\n",
    "    'chicken_curry': 450,\n",
    "    'garlic_bread': 200,\n",
    "    'chicken_wings': 500,\n",
    "    'caprese_salad': 250,\n",
    "    'tacos': 400,\n",
    "    'guacamole': 200,\n",
    "    'filet_mignon': 600,\n",
    "    'nachos': 450,\n",
    "    'ramen': 450,\n",
    "    'hummus': 150,\n",
    "    'spring_rolls': 200,\n",
    "    'onion_rings': 350,\n",
    "    'deviled_eggs': 250,\n",
    "    'ceviche': 200,\n",
    "    'beef_carpaccio': 300,\n",
    "    'pulled_pork_sandwich': 550,\n",
    "    'crab_cakes': 350,\n",
    "    'french_onion_soup': 400,\n",
    "    'clam_chowder': 350,\n",
    "    'lobster_roll_sandwich': 450,\n",
    "    'beef_tartare': 300,\n",
    "    'escargots': 250,\n",
    "    'chocolate_mousse': 350,\n",
    "    'beet_salad': 200,\n",
    "    'gnocchi': 400,\n",
    "    'cannoli': 250,\n",
    "    'bread_pudding': 350,\n",
    "    'escabeche': 300,\n",
    "    'fettuccine_alfredo': 600,\n",
    "    'ceasar_salad': 250,\n",
    "    'poutine': 700,\n",
    "    'shrimp_and_grits': 400,\n",
    "    'foie_gras': 900,\n",
    "    'lobster': 600,\n",
    "    'miso_soup': 100,\n",
    "    'beef_tacos': 450,\n",
    "    'beef_carpaccio': 300,\n",
    "    'beef_tartare': 300,\n",
    "    'beet_salad': 200,\n",
    "    'beignets': 200,\n",
    "    'bibimbap': 450,\n",
    "    'bread_pudding': 350,\n",
    "    'breakfast_burrito': 400,\n",
    "    'bruschetta': 150,\n",
    "    'cannoli': 250,\n",
    "    'caprese_salad': 250,\n",
    "    'carrot_cake': 300,\n",
    "    'ceviche': 200,\n",
    "    'cheese_plate': 300,\n",
    "    'cheesecake': 350,\n",
    "    'chicken_curry': 450,\n",
    "    'chicken_quesadilla': 450,\n",
    "    'chicken_wings': 500,\n",
    "    'chocolate_cake': 500,\n",
    "    'chocolate_mousse': 350,\n",
    "    'churros': 300,\n",
    "    'clam_chowder': 350,\n",
    "    'club_sandwich': 600,\n",
    "    'crab_cakes': 350,\n",
    "    'creme_brulee': 300,\n",
    "    'croque_madame': 450,\n",
    "    'cup_cakes': 250,\n",
    "    'deviled_eggs': 250,\n",
    "    'donuts': 250,\n",
    "    'dumplings': 300,\n",
    "    'edamame': 150,\n",
    "    'eggs_benedict': 500,\n",
    "    'escargots': 250,\n",
    "    'escabeche': 300,\n",
    "    'filet_mignon': 600,\n",
    "    'fish_and_chips': 600,\n",
    "    'foie_gras': 900,\n",
    "    'french_fries': 400,\n",
    "    'french_onion_soup': 400,\n",
    "    'french_toast': 400,\n",
    "    'fried_calamari': 400,\n",
    "    'fried_rice': 350,\n",
    "    'garlic_bread': 200,\n",
    "    'gnocchi': 400,\n",
    "    'greek_salad': 250,\n",
    "    'grilled_cheese_sandwich': 400,\n",
    "    'grilled_salmon': 350,\n",
    "    'guacamole': 200,\n",
    "    'gyoza': 300,\n",
    "    'hamburger': 600,\n",
    "    'hot_and_sour_soup': 250,\n",
    "    'hot_dog': 300,\n",
    "    'huevos_rancheros': 500,\n",
    "    'hummus': 150,\n",
    "    'ice_cream': 200,\n",
    "    'lasagna': 600,\n",
    "    'lobster_bisque': 450,\n",
    "    'lobster_roll_sandwich': 450,\n",
    "    'macaroni_and_cheese': 400,\n",
    "    'miso_soup': 100,\n",
    "    'mussels': 400,\n",
    "    'nachos': 450,\n",
    "    'omelette': 350,\n",
    "    'onion_rings': 350,\n",
    "    'oysters': 200,\n",
    "    'pad_thai': 500,\n",
    "    'paella': 500,\n",
    "    'pancakes': 350,\n",
    "    'panna_cotta': 250,\n",
    "    'peking_duck': 600,\n",
    "    'pho': 450,\n",
    "    'pizza': 800,\n",
    "    'pork_chop': 600,\n",
    "    'poutine': 700,\n",
    "    'prime_rib': 800,\n",
    "    'pulled_pork_sandwich': 550,\n",
    "    'ramen': 450,\n",
    "    'ravioli': 450,\n",
    "    'red_velvet_cake': 350,\n",
    "    'risotto': 450,\n",
    "    'samosa': 300,\n",
    "    'sashimi': 300,\n",
    "    'scallops': 300,\n",
    "    'seaweed_salad': 150,\n",
    "    'shrimp_and_grits': 400,\n",
    "    'spaghetti_bolognese': 450,\n",
    "    'spaghetti_carbonara': 550,\n",
    "    'spring_rolls': 200,\n",
    "    'steak': 700,\n",
    "    'strawberry_shortcake': 300,\n",
    "    'sushi': 400,\n",
    "    'tacos': 400,\n",
    "    'takoyaki': 250,\n",
    "    'tiramisu': 300,\n",
    "    'tuna_tartare': 300,\n",
    "    'waffles': 400\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_calories(image, info):\n",
    "    # Perform food recognition\n",
    "    predicted_class = model.predict(image)\n",
    "    class_label = info.features['label'].int2str(predicted_class.argmax())\n",
    "\n",
    "    # Estimate calorie content\n",
    "    if class_label in calorie_lookup:\n",
    "        return calorie_lookup[class_label]\n",
    "    else:\n",
    "        return \"Calorie information not available.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    # Read image from file\n",
    "    img = tf.io.read_file(file_path)\n",
    "    # Decode image\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Estimated Calories: 350\n"
     ]
    }
   ],
   "source": [
    "# Load an image from file\n",
    "image = load_image(r'C:\\Users\\Lenovo\\tensorflow_datasets\\food101\\train\\downloads\\data.vision\\food-101\\images\\apple_pie\\21063.jpg')\n",
    "\n",
    "# Preprocess the image\n",
    "image_preprocessed, _ = preprocess_image(image, None)\n",
    "\n",
    "# Reshape the preprocessed image to have a batch size of 1\n",
    "image_preprocessed = tf.expand_dims(image_preprocessed, axis=0)\n",
    "\n",
    "# Estimate calories\n",
    "calories = estimate_calories(image_preprocessed, train_info)\n",
    "\n",
    "print(\"Estimated Calories:\", calories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
